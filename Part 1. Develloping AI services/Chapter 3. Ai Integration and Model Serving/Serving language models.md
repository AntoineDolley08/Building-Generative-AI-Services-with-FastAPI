# Chapter 3 ‚Äî AI Integration and Model Serving

> Notes du livre *Building Generative AI Services with FastAPI*

---

## Table des mati√®res

1. [Language Models (Transformers)](#1-language-models-transformers)
2. [Audio Models (Bark)](#2-audio-models-bark)
3. [Vision Models (Stable Diffusion)](#3-vision-models-stable-diffusion)
4. [Video Models](#4-video-models)
5. [3D Models (Shap-E)](#5-3d-models-shap-e)
6. [Strat√©gies de Model Serving](#6-strat√©gies-de-model-serving)
7. [Middleware pour le monitoring](#7-middleware-pour-le-monitoring)
8. [R√©cap & cheat sheets](#8-r√©cap--cheat-sheets)

---

## 1. Language Models (Transformers)

### Transformers vs RNNs

**RNNs (ancien paradigme)** :
- Traitent le texte **s√©quentiellement** (token par token)
- Maintiennent un **state vector** (m√©moire) transportant l'info d'un token au suivant
- **Probl√®me** : plus on avance dans la s√©quence, plus l'impact des premiers tokens diminue ‚Üí perte de contexte
- Entra√Ænement **non parall√©lisable** sur GPU

**Transformers (paradigme actuel)** ‚Äî Paper : *"Attention Is All You Need"* (Vaswani et al., 2017)
- Pas de m√©moire cach√©e (state vector)
- **Self-attention** : mod√©lise les relations entre **tous les mots** (pas juste les voisins)
- Traitement **non-s√©quentiel** ‚Üí parall√©lisable sur GPU ‚Üí scalable

```
RNN :    mot1 ‚Üí mot2 ‚Üí mot3 ‚Üí mot4  (s√©quentiel, perd le contexte)
Transformer : mot1 ‚Üî mot2 ‚Üî mot3 ‚Üî mot4  (tous connect√©s entre eux)
```

### Attention heads

- Blocs sp√©cialis√©s capturant les **relations pairwise** entre mots (attention maps)
- Plusieurs heads par couche ‚Üí analyse sous **diff√©rents angles simultan√©ment**
- Plus de heads/couches = meilleure compr√©hension des patterns complexes

### Pipeline de traitement du texte

**1. Tokenization** ‚Äî Texte ‚Üí tokens ‚Üí IDs num√©riques

```
"FastAPI is great" ‚Üí ["Fast", "API", " is", " great"] ‚Üí [12043, 7112, 374, 2294]
```

**2. Embedding** ‚Äî Tokens ‚Üí vecteurs denses de floats capturant le sens s√©mantique

```
token "Fast" ‚Üí [0.12, -0.34, 0.56, ..., 0.78]  (vecteur de dimension n)
```

Apr√®s entra√Ænement, les mots de sens similaire ont des vecteurs proches.

**3. Positional Encoding** ‚Äî Ajoute l'info d'ordre des mots (les transformers traitent tout en parall√®le)

```
embedding_final = token_embedding + positional_embedding
```

**4. Cosine Similarity** ‚Äî Mesure la similarit√© entre deux mots via l'angle entre vecteurs. Petit angle = sens similaire.

### G√©n√©ration autoregressive

Le transformer pr√©dit le **prochain token** bas√© sur tous les pr√©c√©dents, en boucle jusqu'√† `<stop>` / `<eos>`.

```
Input: "How to set up"
‚Üí pr√©dit "a" ‚Üí "How to set up a"
‚Üí pr√©dit "FastAPI" ‚Üí "How to set up a FastAPI"
‚Üí pr√©dit <eos> ‚Üí stop
```

### Context Window

Nombre max de tokens en m√©moire.

| Mod√®le | Context Window |
|--------|---------------|
| GPT-4o-mini | ~128K tokens (~300 pages) |
| Magic.Dev LTM-2-mini | 100M tokens (~750 romans) |
| Autres mod√®les | Centaines de milliers de tokens |

Trade-offs : window courte ‚Üí perte d'info | window longue ‚Üí plus cher, plus lent sous charge.

### Param√®tres d'inf√©rence cl√©s

| Param√®tre | R√¥le |
|-----------|------|
| `max_new_tokens` | Nombre max de tokens √† g√©n√©rer |
| `do_sample` | `True` = sampling al√©atoire, `False` = greedy (le plus probable) |
| `temperature` | Bas = pr√©cis/d√©terministe, Haut = cr√©atif/al√©atoire |
| `top_k` | Restreint aux K tokens les plus probables |
| `top_p` | Nucleus sampling : garde les tokens couvrant P% de probabilit√© |

### 3 variantes de Transformers

| Variante | Sp√©cialisation | T√¢ches |
|----------|---------------|--------|
| **Encoder-Decoder** | S√©quence ‚Üí s√©quence | Traduction, r√©sum√©, Q&A |
| **Encoder-only** | Compr√©hension du sens | Sentiment analysis, NER, classification |
| **Decoder-only** | Pr√©diction du prochain token | Chatbots, g√©n√©ration de texte |

> üí° Les chatbots (GPT, Llama, Mistral) sont des **decoder-only** transformers.

### Exemple : servir TinyLlama avec FastAPI

```
[Streamlit UI] ‚Üí HTTP GET ‚Üí [FastAPI] ‚Üí [TinyLlama 1.1B]
   client.py                  main.py      models.py
```

```python
# models.py
import torch
from transformers import Pipeline, pipeline

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def load_text_model():
    pipe = pipeline(
        "text-generation",
        model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        torch_dtype=torch.bfloat16, device=device
    )
    return pipe

def generate_text(pipe: Pipeline, prompt: str, temperature: float = 0.7) -> str:
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt},
    ]
    prompt = pipe.tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    predictions = pipe(
        prompt, temperature=temperature,
        max_new_tokens=256, do_sample=True, top_k=50, top_p=0.95,
    )
    return predictions[0]["generated_text"].split("</s>\n<|assistant|>\n")[-1]
```

```python
# main.py
@app.get("/generate/text")
def serve_language_model_controller(prompt: str) -> str:
    pipe = load_text_model()
    return generate_text(pipe, prompt)
```

> ‚ö†Ô∏è Le mod√®le est charg√©/d√©charg√© √† chaque requ√™te ‚Üí anti-pattern. Solution : lifespan (section 6).

### Hardware pour LLMs open-source

| Taille mod√®le | Hardware n√©cessaire |
|---------------|-------------------|
| < 3B (TinyLlama) | CPU possible, GPU recommand√© (~3 GB RAM) |
| < 30B | 1x GPU consumer (RTX 4090, 24 GB VRAM) |
| 70B (quantis√©) | GPU 64 GB VRAM ou multi-GPU |
| 405B-480B (Llama 3.1, Arctic) | 8x H100 (640 GB VRAM total) |

> La plupart des organisations utilisent des mod√®les l√©gers (‚â§ 3B) ou des APIs (OpenAI, Anthropic, Cohere, Mistral).

---

## 2. Audio Models (Bark)

Bark (Suno AI) : transformer capable de g√©n√©rer parole multilingue, musique, bruits de fond, effets sonores.

### Pipeline de synth√®se (4 mod√®les cha√Æn√©s)

```
Texte ‚Üí [1. Semantic] ‚Üí [2. Coarse Acoustics] ‚Üí [3. Fine Acoustics] ‚Üí [4. Encodec] ‚Üí Audio WAV
```

| √âtape | Mod√®le | Type | R√¥le |
|-------|--------|------|------|
| 1 | **Semantic** | Autor√©gressif causal | Capture le sens s√©mantique |
| 2 | **Coarse Acoustics** | Autor√©gressif causal | Features audio brutes |
| 3 | **Fine Acoustics** | Auto-encoder non-causal | Raffine les d√©tails audio |
| 4 | **Encodec** | D√©codeur | D√©code en waveform final |

### Code cl√©

```python
# models.py
from transformers import AutoProcessor, AutoModel

def load_audio_model():
    processor = AutoProcessor.from_pretrained("suno/bark-small", device=device)
    model = AutoModel.from_pretrained("suno/bark-small", device=device)
    return processor, model

def generate_audio(processor, model, prompt, preset):
    inputs = processor(text=[prompt], return_tensors="pt", voice_preset=preset)
    output = model.generate(**inputs, do_sample=True).cpu().numpy().squeeze()
    sample_rate = model.generation_config.sample_rate
    return output, sample_rate
```

```python
# utils.py ‚Äî conversion audio array ‚Üí buffer streamable
import soundfile
from io import BytesIO

def audio_array_to_buffer(audio_array, sample_rate):
    buffer = BytesIO()
    soundfile.write(buffer, audio_array, sample_rate, format="wav")
    buffer.seek(0)
    return buffer
```

```python
# main.py
@app.get("/generate/audio", response_class=StreamingResponse)
def serve_audio_controller(prompt: str, preset: VoicePresets = "v2/en_speaker_1"):
    processor, model = load_audio_model()
    output, sample_rate = generate_audio(processor, model, prompt, preset)
    return StreamingResponse(audio_array_to_buffer(output, sample_rate), media_type="audio/wav")
```

### Concepts cl√©s

- **StreamingResponse** : pour contenus volumineux (audio, vid√©o). Le client consomme au fur et √† mesure.
- **Buffer m√©moire (BytesIO)** > fichier sur disque pour la latence, mais trade-off avec la RAM.
- **Voice Presets** : typ√©s avec `Literal["v2/en_speaker_1", "v2/en_speaker_9"]`

---

## 3. Vision Models (Stable Diffusion)

### Comment fonctionne Stable Diffusion

```
[Image] ‚Üí Encode ‚Üí [Latent Space (bruit blanc)] ‚Üí Denoise (N steps) ‚Üí Decode ‚Üí [Nouvelle Image]
                            ‚Üë
                    [Text Encoder] contr√¥le la g√©n√©ration via le prompt
```

**Entra√Ænement (forward diffusion)** : images encod√©es ‚Üí ajout progressif de bruit ‚Üí le mod√®le apprend √† retirer le bruit.

**Inf√©rence (reverse diffusion)** : bruit al√©atoire ‚Üí d√©bruitage it√©ratif guid√© par le prompt textuel ‚Üí image g√©n√©r√©e.

> Plus d'inference steps = meilleure qualit√©, mais plus lent/co√ªteux.

### Code cl√©

```python
# models.py
from diffusers import DiffusionPipeline

def load_image_model():
    return DiffusionPipeline.from_pretrained(
        "segmind/tiny-sd", torch_dtype=torch.float32, device=device
    )

def generate_image(pipe, prompt):
    return pipe(prompt, num_inference_steps=10).images[0]  # PIL Image
```

```python
# utils.py
def img_to_bytes(image, img_format="PNG") -> bytes:
    buffer = BytesIO()
    image.save(buffer, format=img_format)
    return buffer.getvalue()
```

```python
# main.py
@app.get("/generate/image", response_class=Response)
def serve_image_controller(prompt: str):
    pipe = load_image_model()
    output = generate_image(pipe, prompt)
    return Response(content=img_to_bytes(output), media_type="image/png")
```

### Limitations SD open-source

| Limitation | Description |
|-----------|-------------|
| Coh√©rence | Ne reproduit pas tous les d√©tails du prompt |
| Taille output | Tailles fixes (512√ó512 ou 1024√ó1024) |
| Composabilit√© | Contr√¥le limit√© de la composition |
| Photor√©alisme | D√©tails qui trahissent la g√©n√©ration IA |
| Texte lisible | Certains mod√®les √©chouent |

### LoRA (Low-Rank Adaptation)

Technique de fine-tuning efficace : ajoute un **minimum de param√®tres entra√Ænables** par couche, les param√®tres originaux restent fig√©s. R√©duit drastiquement la m√©moire GPU n√©cessaire.

---

## 4. Video Models

G√©n√©rer 1 seconde de vid√©o = des dizaines de frames ‚Üí **GPU quasi obligatoire**.

### Pipeline image-to-video (Stability AI)

```
[Image PIL] ‚Üí resize(1024√ó576) ‚Üí [Stable Video Diffusion] ‚Üí [Frames PIL] ‚Üí [av/ffmpeg] ‚Üí MP4 stream
```

```python
# models.py
from diffusers import StableVideoDiffusionPipeline

def load_video_model():
    return StableVideoDiffusionPipeline.from_pretrained(
        "stabilityai/stable-video-diffusion-img2vid",
        torch_dtype=torch.float16, variant="fp16", device=device,
    )

def generate_video(pipe, image, num_frames=25):
    image = image.resize((1024, 576))
    generator = torch.manual_seed(42)
    return pipe(image, decode_chunk_size=8, generator=generator, num_frames=num_frames).frames[0]
```

### Export frames ‚Üí MP4

```python
import av

def export_to_video_buffer(images):
    buffer = BytesIO()
    output = av.open(buffer, "w", format="mp4")
    stream = output.add_stream("h264", 30)        # H.264, 30 FPS
    stream.pix_fmt = "yuv444p"                     # full color resolution
    stream.options = {"crf": "17"}                 # quasi-lossless
    for image in images:
        output.mux(stream.encode(av.VideoFrame.from_image(image)))
    output.mux(stream.encode(None))                # flush
    return buffer
```

```python
# main.py ‚Äî premier endpoint POST avec File upload
@app.post("/generate/video", response_class=StreamingResponse)
def serve_video_controller(image: bytes = File(...), num_frames: int = 25):
    image = Image.open(BytesIO(image))
    model = load_video_model()
    frames = generate_video(model, image, num_frames)
    return StreamingResponse(export_to_video_buffer(frames), media_type="video/mp4")
```

### OpenAI Sora ‚Äî Vision Transformer + Diffusion

Combine **Transformer** (scalabilit√©, d√©pendances long-range) + **Diffusion** (qualit√©, contr√¥le fin).

```
LLM : pr√©dit le prochain TOKEN dans une s√©quence texte
Sora : pr√©dit le prochain PATCH dans une s√©quence vid√©o
```

Innovations : **3D U-Net** (3e dimension = temps), compression en **space-time patches**, g√©n√©ration en taille d'√©cran native.

**Capacit√©s √©mergentes** :

| Capacit√© | Description |
|----------|-------------|
| 3D consistency | Objets coh√©rents quand la cam√©ra bouge |
| Object permanence | Objets persistants hors-champ |
| World interaction | Actions affectent l'environnement |
| World simulation | Simule des mondes avec r√®gles physiques |

---

## 5. 3D Models (Shap-E)

### Vocabulaire 3D

- **Vertices** : points (x, y, z) | **Edges** : segments entre vertices | **Faces** : polygones | **Mesh** : ensemble vertices + edges + faces

### Pipeline Shap-E

```
[Prompt] ‚Üí [Encoder ‚Üí Implicit Functions] ‚Üí [NeRF rendering] ‚Üí [SDF ‚Üí Mesh] ‚Üí OBJ file
```

| Composant | R√¥le |
|-----------|------|
| **Implicit functions** | D√©finissent surfaces/volumes en continu |
| **NeRF** | Construit la sc√®ne 3D : coordonn√©e + direction ‚Üí densit√© + couleur RGB |
| **SDF** | Convertit en mesh. Distance : n√©gatif=int√©rieur, 0=surface, positif=ext√©rieur |

```python
# models.py
from diffusers import ShapEPipeline

def generate_3d_geometry(pipe, prompt, num_inference_steps):
    return pipe(
        prompt, guidance_scale=15.0,
        num_inference_steps=num_inference_steps, output_type="mesh",
    ).images[0]
```

```python
# main.py ‚Äî avec header Content-Disposition pour forcer le t√©l√©chargement
@app.get("/generate/3d", response_class=StreamingResponse)
def serve_3d_controller(prompt: str, num_inference_steps: int = 25):
    model = load_3d_model()
    mesh = generate_3d_geometry(model, prompt, num_inference_steps)
    response = StreamingResponse(mesh_to_obj_buffer(mesh), media_type="model/obj")
    response.headers["Content-Disposition"] = f"attachment; filename={prompt}.obj"
    return response
```

---

## 6. Strat√©gies de Model Serving

### Strat√©gie 1 : Model Agnostic (load/unload par requ√™te)

```
Requ√™te ‚Üí Load model ‚Üí Inf√©rence ‚Üí Unload model ‚Üí R√©ponse
```

‚úÖ Swap dynamique de mod√®les | ‚ùå Tr√®s lent, FIFO blocking | **Prototypage uniquement, jamais en prod.**

### Strat√©gie 2 : ‚≠ê Preload avec Lifespan (recommand√©)

```
App startup ‚Üí Load model ‚Üí [Requ√™te 1, 2, 3...] ‚Üí App shutdown ‚Üí Unload + cleanup
```

```python
from contextlib import asynccontextmanager

models = {}

@asynccontextmanager
async def lifespan(_: FastAPI):
    models["text2image"] = load_image_model()  # startup
    yield                                       # handle requests
    models.clear()                              # shutdown/cleanup

app = FastAPI(lifespan=lifespan)

@app.get("/generate/image")
def serve_image(prompt: str):
    output = generate_image(models["text2image"], prompt)
    return Response(content=img_to_bytes(output), media_type="image/png")
```

‚úÖ Pas de reload, r√©ponses rapides | ‚ùå RAM/VRAM occup√©e en permanence

> Legacy : `@app.on_event("startup")` / `@app.on_event("shutdown")` ‚Äî d√©pr√©ci√©.

### Strat√©gie 3 : Serving externe (FastAPI = couche logique)

FastAPI g√®re auth, coordination, monitoring. Le mod√®le tourne **ailleurs**.

**Option A ‚Äî BentoML (self-hosted)** :

```python
# bento.py
@bentoml.service(resources={"cpu": "4"}, traffic={"timeout": 120})
class Generate:
    def __init__(self):
        self.pipe = load_image_model()

    @bentoml.api(route="/generate/image")
    def generate(self, prompt: str):
        return self.pipe(prompt, num_inference_steps=10).images[0]
```

```python
# main.py (FastAPI = client HTTP)
async def serve_bentoml_controller(prompt: str):
    async with httpx.AsyncClient() as client:
        response = await client.post("http://localhost:5000/generate", json={"prompt": prompt})
    return Response(content=response.content, media_type="image/png")
```

**Option B ‚Äî API providers (OpenAI, Anthropic, etc.)** :

```python
openai_client = OpenAI()

@app.get("/generate/openai/text")
def serve_openai(prompt: str):
    response = openai_client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "system", "content": system_prompt},
                  {"role": "user", "content": prompt}],
    )
    return response.choices[0].message.content
```

**Option C ‚Äî LangChain (abstraction multi-providers)** :

```python
from langchain.chains.llm import LLMChain
from langchain_openai import OpenAI

llm = OpenAI()
llm_chain = LLMChain(prompt=prompt_template, llm=llm)

@app.get("/generate/text")
def generate(query: str):
    return llm_chain.run(query)
```

### Tableau comparatif des strat√©gies

| Strat√©gie | Quand l'utiliser | Performance |
|-----------|-----------------|-------------|
| **Model Agnostic** | Prototypage, swap de mod√®les | ‚ùå Lent |
| **Lifespan Preload** | Production, mod√®le unique | ‚úÖ Rapide |
| **Externe (BentoML)** | Gros mod√®les, GPU d√©di√© | ‚úÖ Scalable |
| **Externe (API provider)** | Pas de GPU, budget API ok | ‚úÖ Simple |
| **Externe (LangChain)** | Multi-providers, flexibilit√© | ‚úÖ Flexible |

> ‚ö†Ô∏è Serving externe = donn√©es partag√©es avec le provider. Data privacy critique ‚Üí self-host ou cloud manag√© (Azure OpenAI).

---

## 7. Middleware pour le monitoring

```python
@app.middleware("http")
async def monitor_service(req: Request, call_next) -> Response:
    request_id = uuid4().hex
    start_time = time.perf_counter()

    response = await call_next(req)

    response_time = round(time.perf_counter() - start_time, 4)
    response.headers["X-Response-Time"] = str(response_time)
    response.headers["X-API-Request-ID"] = request_id

    # Log (en prod ‚Üí BDD, cf Chapter 7)
    log_to_csv(request_id, req.url, req.client.host, response_time, response.status_code)
    return response
```

| Champ | Source |
|-------|--------|
| Request ID | `uuid4()` |
| Datetime | `datetime.now(UTC)` |
| Endpoint | `req.url` |
| Client IP | `req.client.host` |
| Response time | `perf_counter()` delta |
| Status code | `response.status_code` |

Points importants :
- S'ex√©cute **avant et apr√®s** chaque handler ‚Üí pas besoin de logger dans chaque endpoint
- En prod : persister en BDD (pas CSV, containers √©ph√©m√®res)
- Logger les bodies ‚Üí attention data privacy et performance

---

## 8. R√©cap & cheat sheets

### Response patterns FastAPI

| Contenu | Method | Input | Response type | Media type | Lib |
|---------|--------|-------|--------------|------------|-----|
| Texte/JSON | GET | Query params | `return {...}` | `application/json` | ‚Äî |
| Image | GET | Query params | `Response(bytes)` | `image/png` | Pillow |
| Audio | GET | Query params | `StreamingResponse(buffer)` | `audio/wav` | soundfile |
| Vid√©o | POST | File upload | `StreamingResponse(buffer)` | `video/mp4` | av/ffmpeg |
| 3D | GET | Query params | `StreamingResponse(buffer)` | `model/obj` | open3d |

### D√©pendances (toutes)

```bash
# Core
uv add "fastapi[standard]" uvicorn openai

# ML/AI
uv add transformers torch diffusers

# Audio/Video/3D
uv add soundfile av open3d python-multipart pillow

# Optionnel
uv add accelerate         # optimise l'usage m√©moire CPU
uv add bentoml            # serving externe
uv add langchain langchain-openai  # abstraction multi-providers
uv add streamlit          # UI de prototypage
uv add httpx              # client HTTP async
```

### Points cl√©s du chapitre

1. **Tokenization ‚Üí Embedding ‚Üí Positional Encoding ‚Üí Attention ‚Üí Pr√©diction autoregressive** = pipeline complet des LLMs
2. **Stable Diffusion** = encode ‚Üí noise ‚Üí denoise guid√© par texte ‚Üí decode
3. **Sora** = Transformer + Diffusion avec 3D U-Net et space-time patches
4. **Shap-E** = fonctions implicites + NeRF + SDF pour la 3D
5. **Lifespan preload** = LE pattern de production pour le model serving
6. **FastAPI comme couche logique** + serving externe (BentoML/API) pour les gros mod√®les
7. **Middleware** = monitoring centralis√© sans toucher aux handlers